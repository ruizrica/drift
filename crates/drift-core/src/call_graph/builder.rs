//! Streaming Call Graph Builder
//!
//! Memory-optimized call graph builder that writes shards incrementally
//! instead of accumulating the entire graph in memory.
//!
//! Key features:
//! - Process one file at a time
//! - Write each file's functions to SQLite immediately
//! - Run resolution pass in batches after all files processed
//! - Disk-backed function index prevents OOM on large codebases
//! - Data access detection integrated (Prisma, Supabase, TypeORM, etc.)
//!
//! Two build modes:
//! - `build()` - Legacy JSON shard mode (backward compatible)
//! - `build_sqlite()` - New SQLite mode with parallel parsing (recommended)

use std::collections::HashMap;
use std::fs;
use std::io::{BufRead, BufReader, BufWriter, Write};
use std::path::PathBuf;
use std::time::Instant;

use rayon::prelude::*;

use crate::parsers::{ParserManager, Language};
use crate::scanner::{Scanner, ScanConfig};
use crate::boundaries::DataAccessDetector;
use super::types::*;
use super::extractor::to_function_entries;
use super::universal_extractor::UniversalExtractor;
use super::storage::{ParallelWriter, FunctionBatch};

/// Configuration for the streaming builder
pub struct BuilderConfig {
    /// Project root directory
    pub root_dir: PathBuf,
    /// Batch size for resolution pass
    pub resolution_batch_size: usize,
    /// Progress callback
    pub on_progress: Option<Box<dyn Fn(usize, usize, &str) + Send + Sync>>,
}

impl Default for BuilderConfig {
    fn default() -> Self {
        Self {
            root_dir: PathBuf::from("."),
            resolution_batch_size: 50,
            on_progress: None,
        }
    }
}

/// Streaming call graph builder
pub struct StreamingBuilder {
    config: BuilderConfig,
    parser: ParserManager,
    extractor: UniversalExtractor,
    shards_dir: PathBuf,
    resolution_index_path: PathBuf,
}

impl StreamingBuilder {
    pub fn new(config: BuilderConfig) -> Self {
        let drift_dir = config.root_dir.join(".drift");
        let shards_dir = drift_dir.join("lake").join("callgraph").join("files");
        let resolution_index_path = drift_dir.join("lake").join("callgraph").join("resolution-index.ndjson");
        
        Self {
            config,
            parser: ParserManager::new(),
            extractor: UniversalExtractor::new(),
            shards_dir,
            resolution_index_path,
        }
    }
    
    /// Get the SQLite database path
    fn db_path(&self) -> PathBuf {
        self.config.root_dir
            .join(".drift")
            .join("lake")
            .join("callgraph")
            .join("callgraph.db")
    }

    /// Build call graph using SQLite storage with parallel parsing
    /// 
    /// This is the recommended method for large codebases (1000+ files).
    /// Uses rayon for parallel parsing and MPSC channel for batched SQLite writes.
    /// 
    /// Performance characteristics:
    /// - O(n) time complexity (linear with file count)
    /// - O(1) memory usage (constant regardless of codebase size)
    /// - ~0.5ms per file on modern hardware
    pub fn build_sqlite(&self, patterns: &[&str]) -> BuildResult {
        let start = Instant::now();
        let mut errors: Vec<String> = Vec::new();
        
        // Ensure directories exist
        let db_path = self.db_path();
        if let Some(parent) = db_path.parent() {
            fs::create_dir_all(parent).ok();
        }
        
        // Find all matching files
        let scanner = Scanner::new(ScanConfig {
            root: self.config.root_dir.clone(),
            patterns: patterns.iter().map(|s| s.to_string()).collect(),
            ..Default::default()
        });
        
        let scan_result = scanner.scan();
        let files: Vec<_> = scan_result.files.iter()
            .filter(|f| Language::from_path(&f.path).is_some())
            .collect();
        
        let total_files = files.len();
        
        // Create parallel writer (spawns background thread)
        let writer = ParallelWriter::new(db_path.clone(), 100); // Batch size of 100 files
        let sender = writer.sender();
        
        // Track progress atomically
        let progress_counter = std::sync::atomic::AtomicUsize::new(0);
        let root_dir = self.config.root_dir.clone();
        let on_progress = &self.config.on_progress;
        
        // Parallel parsing with rayon
        let parse_errors: Vec<String> = files
            .par_iter()
            .filter_map(|file_info| {
                // Progress callback
                let current = progress_counter.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
                if let Some(ref cb) = on_progress {
                    cb(current + 1, total_files, &file_info.path);
                }
                
                // Process file
                match Self::process_file_static(&root_dir, &file_info.path) {
                    Ok(Some(batch)) => {
                        // Send to writer thread
                        if sender.send(batch).is_err() {
                            Some(format!("{}: Writer channel closed", file_info.path))
                        } else {
                            None
                        }
                    }
                    Ok(None) => None, // No functions in file
                    Err(e) => Some(format!("{}: {}", file_info.path, e)),
                }
            })
            .collect();
        
        errors.extend(parse_errors);
        
        // Drop sender to signal writer to finish, then wait for completion
        drop(sender);
        let stats = match writer.finish() {
            Ok(s) => s,
            Err(e) => {
                errors.push(format!("SQLite error: {}", e));
                super::storage::DbStats::default()
            }
        };
        
        let resolution_rate = if stats.total_calls > 0 {
            stats.resolved_calls as f32 / stats.total_calls as f32
        } else {
            0.0
        };
        
        BuildResult {
            files_processed: total_files,
            total_functions: stats.total_functions,
            total_calls: stats.total_calls,
            resolved_calls: stats.resolved_calls,
            resolution_rate,
            entry_points: stats.entry_points,
            data_accessors: stats.data_accessors,
            errors,
            duration_ms: start.elapsed().as_millis() as u64,
        }
    }
    
    /// Process a single file (static version for parallel use)
    fn process_file_static(root_dir: &PathBuf, file: &str) -> Result<Option<FunctionBatch>, String> {
        use std::cell::RefCell;
        
        // Thread-local parser, extractor, and data access detector to avoid re-initialization overhead
        thread_local! {
            static PARSER: RefCell<ParserManager> = RefCell::new(ParserManager::new());
            static EXTRACTOR: UniversalExtractor = UniversalExtractor::new();
            static DATA_ACCESS_DETECTOR: DataAccessDetector = DataAccessDetector::new();
        }
        
        let full_path = root_dir.join(file);
        
        // Read source
        let source = fs::read_to_string(&full_path)
            .map_err(|e| format!("Failed to read file: {}", e))?;
        
        // Parse using thread-local parser
        let parse_result = PARSER.with(|parser| {
            parser.borrow_mut().parse_file(file, &source)
        }).ok_or_else(|| "Unsupported language".to_string())?;
        
        // Extract functions and calls using thread-local extractor
        let extraction = EXTRACTOR.with(|extractor| {
            extractor.extract_from_parse_result(&parse_result)
        });
        
        if extraction.functions.is_empty() {
            return Ok(None);
        }
        
        // Extract data access points using thread-local detector
        // AST-first: detect from parsed call sites (Prisma, Supabase, TypeORM, etc.)
        let mut data_access = DATA_ACCESS_DETECTOR.with(|detector| {
            detector.detect_from_ast(&parse_result, file)
        });
        
        // Fallback: detect SQL in raw source (for embedded SQL strings)
        let sql_access = DATA_ACCESS_DETECTOR.with(|detector| {
            detector.detect_sql_in_source(&source, file)
        });
        data_access.extend(sql_access);
        
        // Convert DataAccessPoint to DataAccessRef for function entries
        let data_access_refs: Vec<DataAccessRef> = data_access
            .into_iter()
            .map(|da| DataAccessRef {
                table: da.table,
                operation: match da.operation {
                    crate::boundaries::DataOperation::Read => DataOperation::Read,
                    crate::boundaries::DataOperation::Write => DataOperation::Write,
                    crate::boundaries::DataOperation::Delete => DataOperation::Delete,
                },
                fields: da.fields,
                line: da.line,
            })
            .collect();
        
        // Convert to function entries with data access
        let functions = to_function_entries(file, &extraction, &data_access_refs);
        
        Ok(Some(FunctionBatch {
            file: file.to_string(),
            functions,
        }))
    }

    /// Build call graph with streaming/sharded storage (legacy JSON mode)
    pub fn build(&mut self, patterns: &[&str]) -> BuildResult {
        let start = Instant::now();
        let mut errors = Vec::new();
        
        // Ensure directories exist
        fs::create_dir_all(&self.shards_dir).ok();
        
        // Find all matching files
        let scanner = Scanner::new(ScanConfig {
            root: self.config.root_dir.clone(),
            patterns: patterns.iter().map(|s| s.to_string()).collect(),
            ..Default::default()
        });
        
        let scan_result = scanner.scan();
        let files: Vec<_> = scan_result.files.iter()
            .filter(|f| Language::from_path(&f.path).is_some())
            .collect();
        
        let total_files = files.len();
        let mut total_functions = 0;
        let mut total_calls = 0;
        let mut entry_points = 0;
        let mut data_accessors = 0;
        
        // Phase 1: Extract and save shards
        for (i, file_info) in files.iter().enumerate() {
            if let Some(ref cb) = self.config.on_progress {
                cb(i + 1, total_files, &file_info.path);
            }
            
            match self.process_file(&file_info.path) {
                Ok(Some(shard)) => {
                    // Update stats
                    total_functions += shard.functions.len();
                    total_calls += shard.functions.iter()
                        .map(|f| f.calls.len())
                        .sum::<usize>();
                    entry_points += shard.functions.iter()
                        .filter(|f| f.is_entry_point)
                        .count();
                    data_accessors += shard.functions.iter()
                        .filter(|f| f.is_data_accessor)
                        .count();
                    
                    // Save shard
                    if let Err(e) = self.save_shard(&shard) {
                        errors.push(format!("{}: {}", file_info.path, e));
                    }
                }
                Ok(None) => {} // No functions in file
                Err(e) => {
                    errors.push(format!("{}: {}", file_info.path, e));
                }
            }
        }
        
        // Phase 2: Resolution pass
        let resolved_calls = self.run_resolution_pass();
        
        let resolution_rate = if total_calls > 0 {
            resolved_calls as f32 / total_calls as f32
        } else {
            0.0
        };
        
        BuildResult {
            files_processed: total_files,
            total_functions,
            total_calls,
            resolved_calls,
            resolution_rate,
            entry_points,
            data_accessors,
            errors,
            duration_ms: start.elapsed().as_millis() as u64,
        }
    }

    /// Process a single file and return its shard
    fn process_file(&mut self, file: &str) -> Result<Option<CallGraphShard>, String> {
        let full_path = self.config.root_dir.join(file);
        
        // Read source
        let source = fs::read_to_string(&full_path)
            .map_err(|e| format!("Failed to read file: {}", e))?;
        
        // Parse
        let parse_result = self.parser.parse_file(file, &source)
            .ok_or_else(|| "Unsupported language".to_string())?;
        
        // Extract functions and calls
        let extraction = self.extractor.extract_from_parse_result(&parse_result);
        
        if extraction.functions.is_empty() {
            return Ok(None);
        }
        
        // Extract data access points
        let data_detector = DataAccessDetector::new();
        
        // AST-first: detect from parsed call sites
        let mut data_access = data_detector.detect_from_ast(&parse_result, file);
        
        // Fallback: detect SQL in raw source
        let sql_access = data_detector.detect_sql_in_source(&source, file);
        data_access.extend(sql_access);
        
        // Convert DataAccessPoint to DataAccessRef
        let data_access_refs: Vec<DataAccessRef> = data_access
            .into_iter()
            .map(|da| DataAccessRef {
                table: da.table,
                operation: match da.operation {
                    crate::boundaries::DataOperation::Read => DataOperation::Read,
                    crate::boundaries::DataOperation::Write => DataOperation::Write,
                    crate::boundaries::DataOperation::Delete => DataOperation::Delete,
                },
                fields: da.fields,
                line: da.line,
            })
            .collect();
        
        // Convert to function entries with data access
        let functions = to_function_entries(file, &extraction, &data_access_refs);
        
        Ok(Some(CallGraphShard {
            file: file.to_string(),
            functions,
        }))
    }
    
    /// Save a shard to disk
    fn save_shard(&self, shard: &CallGraphShard) -> Result<(), String> {
        let hash = self.hash_file_path(&shard.file);
        let shard_path = self.shards_dir.join(format!("{}.json", hash));
        
        let json = serde_json::to_string_pretty(shard)
            .map_err(|e| format!("Failed to serialize: {}", e))?;
        
        fs::write(&shard_path, json)
            .map_err(|e| format!("Failed to write shard: {}", e))?;
        
        Ok(())
    }
    
    /// Load a shard from disk
    fn load_shard(&self, file_hash: &str) -> Option<CallGraphShard> {
        let shard_path = self.shards_dir.join(format!("{}.json", file_hash));
        let content = fs::read_to_string(&shard_path).ok()?;
        serde_json::from_str(&content).ok()
    }
    
    /// List all shard file hashes
    fn list_shards(&self) -> Vec<String> {
        fs::read_dir(&self.shards_dir)
            .map(|entries| {
                entries
                    .filter_map(|e| e.ok())
                    .filter_map(|e| {
                        let name = e.file_name().to_string_lossy().to_string();
                        if name.ends_with(".json") {
                            Some(name.trim_end_matches(".json").to_string())
                        } else {
                            None
                        }
                    })
                    .collect()
            })
            .unwrap_or_default()
    }
    
    /// Hash a file path to create shard filename
    fn hash_file_path(&self, file: &str) -> String {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        
        let mut hasher = DefaultHasher::new();
        file.hash(&mut hasher);
        format!("{:012x}", hasher.finish())
    }

    /// Run resolution pass across all shards
    fn run_resolution_pass(&mut self) -> usize {
        let file_hashes = self.list_shards();
        
        // Phase 1: Build disk-backed function index
        if let Err(e) = self.build_resolution_index(&file_hashes) {
            eprintln!("Failed to build resolution index: {}", e);
            return 0;
        }
        
        // Phase 2: Load index into memory
        let (function_index, function_files) = match self.load_resolution_index() {
            Ok(idx) => idx,
            Err(e) => {
                eprintln!("Failed to load resolution index: {}", e);
                return 0;
            }
        };
        
        // Phase 3: Resolve calls in batches
        let mut total_resolved = 0;
        
        for batch in file_hashes.chunks(self.config.resolution_batch_size) {
            for file_hash in batch {
                if let Some(mut shard) = self.load_shard(file_hash) {
                    let mut modified = false;
                    
                    for func in &mut shard.functions {
                        for call in &mut func.calls {
                            let resolution = self.resolve_call(
                                &call.target,
                                &shard.file,
                                &function_index,
                                &function_files,
                            );
                            
                            call.resolved = resolution.resolved;
                            call.confidence = resolution.confidence;
                            call.resolved_id = resolution.resolved_id;
                            
                            if resolution.resolved {
                                total_resolved += 1;
                            }
                            modified = true;
                        }
                    }
                    
                    if modified {
                        let _ = self.save_shard(&shard);
                    }
                }
            }
        }
        
        // Cleanup
        let _ = fs::remove_file(&self.resolution_index_path);
        
        total_resolved
    }
    
    /// Build the resolution index to disk (NDJSON format)
    fn build_resolution_index(&self, file_hashes: &[String]) -> Result<(), String> {
        if let Some(parent) = self.resolution_index_path.parent() {
            fs::create_dir_all(parent)
                .map_err(|e| format!("Failed to create directory: {}", e))?;
        }
        
        let file = fs::File::create(&self.resolution_index_path)
            .map_err(|e| format!("Failed to create index file: {}", e))?;
        let mut writer = BufWriter::new(file);
        
        for file_hash in file_hashes {
            if let Some(shard) = self.load_shard(file_hash) {
                for func in &shard.functions {
                    let entry = ResolutionEntry {
                        name: func.name.clone(),
                        id: func.id.clone(),
                        file: shard.file.clone(),
                    };
                    
                    let json = serde_json::to_string(&entry)
                        .map_err(|e| format!("Failed to serialize: {}", e))?;
                    writeln!(writer, "{}", json)
                        .map_err(|e| format!("Failed to write: {}", e))?;
                }
            }
        }
        
        writer.flush().map_err(|e| format!("Failed to flush: {}", e))?;
        Ok(())
    }

    /// Load the resolution index from disk
    fn load_resolution_index(&self) -> Result<(HashMap<String, Vec<String>>, HashMap<String, String>), String> {
        let mut function_index: HashMap<String, Vec<String>> = HashMap::new();
        let mut function_files: HashMap<String, String> = HashMap::new();
        
        let file = fs::File::open(&self.resolution_index_path)
            .map_err(|e| format!("Failed to open index: {}", e))?;
        let reader = BufReader::new(file);
        
        for line in reader.lines() {
            let line = line.map_err(|e| format!("Failed to read line: {}", e))?;
            if line.trim().is_empty() {
                continue;
            }
            
            if let Ok(entry) = serde_json::from_str::<ResolutionEntry>(&line) {
                // Build name -> [ids] index
                function_index
                    .entry(entry.name.clone())
                    .or_default()
                    .push(entry.id.clone());
                
                // Build id -> file index
                function_files.insert(entry.id, entry.file);
            }
        }
        
        Ok((function_index, function_files))
    }
    
    /// Resolve a call to its target function
    fn resolve_call(
        &self,
        target: &str,
        caller_file: &str,
        function_index: &HashMap<String, Vec<String>>,
        function_files: &HashMap<String, String>,
    ) -> Resolution {
        let candidates = match function_index.get(target) {
            Some(c) if !c.is_empty() => c,
            _ => return Resolution::unresolved(),
        };
        
        // Strategy 1: Same file (highest confidence)
        let same_file: Vec<_> = candidates
            .iter()
            .filter(|id| function_files.get(*id).map(|f| f == caller_file).unwrap_or(false))
            .collect();
        
        if same_file.len() == 1 {
            return Resolution {
                resolved: true,
                resolved_id: Some(same_file[0].clone()),
                confidence: 0.95,
            };
        }
        
        // Strategy 2: Single candidate globally
        if candidates.len() == 1 {
            return Resolution {
                resolved: true,
                resolved_id: Some(candidates[0].clone()),
                confidence: 0.8,
            };
        }
        
        // Strategy 3: Multiple candidates - pick same file if available
        if !same_file.is_empty() {
            return Resolution {
                resolved: true,
                resolved_id: Some(same_file[0].clone()),
                confidence: 0.7,
            };
        }
        
        // Strategy 4: Multiple candidates, different files - low confidence
        Resolution {
            resolved: true,
            resolved_id: Some(candidates[0].clone()),
            confidence: 0.4,
        }
    }
}

/// Resolution result
struct Resolution {
    resolved: bool,
    resolved_id: Option<String>,
    confidence: f32,
}

impl Resolution {
    fn unresolved() -> Self {
        Self {
            resolved: false,
            resolved_id: None,
            confidence: 0.0,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_hash_file_path() {
        let config = BuilderConfig {
            root_dir: PathBuf::from("."),
            ..Default::default()
        };
        let builder = StreamingBuilder::new(config);
        
        let hash1 = builder.hash_file_path("src/main.ts");
        let hash2 = builder.hash_file_path("src/main.ts");
        let hash3 = builder.hash_file_path("src/other.ts");
        
        assert_eq!(hash1, hash2);
        assert_ne!(hash1, hash3);
        // Hash is hex-encoded u64, so 16 chars
        assert!(hash1.len() >= 12);
    }
}
